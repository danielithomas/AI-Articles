# Definitions and Acronyms

This document provides definitions for core Artificial Intelligence concepts, technical terms, and acronyms referenced throughout the AI Articles collection. Terms are organised by thematic category to align with the article structure.

---

## Strategy & Governance

|Term|Definition|
|---|---|
|AI Ethics Principles|A set of guidelines and values that govern the responsible development and deployment of AI systems. Common principles include fairness, transparency, accountability, privacy, safety, and human oversight. Various organisations and governments have published their own frameworks.|
|AI Governance|The framework of policies, processes, and organisational structures that ensure AI systems are developed, deployed, and managed responsibly. Encompasses risk management, compliance, accountability mechanisms, and stakeholder oversight throughout the AI lifecycle.|
|Data Provenance|The documented history and lineage of data, including its origin, transformations, and processing steps. In AI contexts, provenance tracking ensures transparency about training data sources and helps identify potential biases or quality issues.|
|EU AI Act|The European Union's comprehensive regulatory framework for artificial intelligence, establishing risk-based requirements for AI systems. Classifies AI applications into risk categories (unacceptable, high, limited, minimal) with corresponding compliance obligations. Fines can reach up to 7% of global turnover.|
|Human Oversight|The principle that humans should maintain meaningful control over AI systems, particularly for high-stakes decisions. Includes the ability to monitor, intervene, correct, or override AI outputs. A core requirement in responsible AI frameworks and regulations.|
|ISO/IEC 42001|The international standard for AI Management Systems, providing a framework for organisations to establish, implement, maintain, and continually improve their AI management practices. Addresses governance, risk management, and responsible AI development.|
|Responsible AI|A framework and set of practices ensuring AI systems are developed and deployed ethically, transparently, and in alignment with societal values. Key principles include fairness, accountability, transparency, privacy, and human oversight. See: [CSIRO Responsible AI Pattern Catalogue](https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/)|

---

## Security & Risk

|Term|Definition|
|---|---|
|Adversarial Attack|A technique where malicious inputs are crafted to deceive AI models into making incorrect predictions or classifications. These attacks exploit vulnerabilities in how models process data, often using imperceptible perturbations to images or carefully constructed text.|
|AI Risk|The potential for AI systems to cause unintended harm, including safety failures, bias amplification, security vulnerabilities, misuse, or broader societal impacts. Risk management frameworks help organisations identify, assess, and mitigate these concerns. See: [MIT AI Risk Repository](https://airisk.mit.edu/)|
|Data Poisoning|An attack that compromises AI systems by introducing malicious or corrupted data into training datasets. Poisoned data can cause models to learn incorrect patterns, create backdoors, or degrade performance. Particularly concerning because effects persist after training.|
|Jailbreaking|A technique that attempts to bypass an AI model's safety guidelines and content restrictions through carefully crafted prompts. Unlike prompt injection (which targets applications), jailbreaking specifically targets the model's built-in safeguards to generate restricted content.|
|Knowledge Dilution|A phenomenon where LLM domain-specific expertise systematically degrades when exposed to large volumes of irrelevant but contextually plausible information. Research shows security feature implementation can drop by 47% as context dilution increases, with complex multi-step tasks showing greater vulnerability.|
|Lethal Trifecta|A security concept describing the dangerous combination of three factors in AI systems: access to sensitive data, exposure to untrusted content, and the ability to communicate externally. When all three are present, attackers can craft inputs that exfiltrate sensitive information.|
|Model Inversion|An attack that attempts to reconstruct training data or extract sensitive information from a trained model by analysing its outputs. Attackers query the model systematically to infer characteristics of the data it was trained on.|
|Prompt Injection|A security vulnerability where attackers craft inputs that manipulate LLM behaviour by disguising malicious instructions as legitimate user input. Ranked as the top security risk on the OWASP Top 10 for LLM Applications. Can be direct (user-controlled) or indirect (embedded in external content). See: [OWASP LLM01](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)|
|Red Teaming|A security testing methodology where experts deliberately attempt to find vulnerabilities, bypass safeguards, or cause unintended behaviours in AI systems. Used to identify weaknesses before deployment and to stress-test safety measures through adversarial probing.|
|Shadow Agent|An AI agent deployed within an organisation without official approval or oversight. Shadow agents create governance and security risks as they operate outside established controls, policies, and monitoring systems.|

---

## Architecture & Operations

|Term|Definition|
|---|---|
|Agent|An autonomous entity in AI that observes inputs, applies reasoning or learning, and acts to maximise outcomes within its environment. Key components include: perception (collecting data from sensors or inputs), decision-making (using algorithms or learned models to choose actions), and action (executing tasks such as answering questions or controlling systems).|
|Agent Scaling Laws|Quantitative principles describing how agent system performance changes with model capability, coordination architecture, and task properties. Unlike neural scaling laws (which follow power laws requiring massive parameter increases), collaborative scaling exhibits logistic growth patterns at smaller scales. Research shows task decomposability and coordination overhead are stronger predictors than simple agent count.|
|Agentic AI|AI systems designed to operate with significant autonomy, making decisions and taking actions with minimal human intervention. These systems can plan, reason, and adapt their behaviour to achieve goals across extended task sequences. Distinguished from simple chatbots by their ability to use tools and maintain state.|
|AI Tech Stack|A layered framework for understanding AI system architecture, comprising: Governance (rules and protocols), Application (user interfaces), Infrastructure (computational foundation), Model (core algorithms), and Data (foundational raw material). Each layer presents distinct security considerations.|
|Context Decay|The degradation of AI model performance as conversation or context length increases, typically becoming noticeable beyond approximately 32,000 tokens. Causes models to "forget" earlier instructions or information, reducing response quality and coherence.|
|Context Engineering|The practice of strategically managing and optimising the information provided to AI models within their context window. Has emerged as a core skill for AI practitioners in 2025, replacing simple prompt engineering for complex agent architectures.|
|Context Window|The maximum amount of text (measured in tokens) that an AI model can process in a single interaction. Includes both the input prompt and the generated response. Modern models range from 8K to 200K+ tokens. Larger windows enable more complex tasks but increase computational costs.|
|Control Plane|A centralised management layer for deploying, organising, and governing AI agents at enterprise scale. Provides capabilities including agent registry, access control, visualisation, interoperability, and security. Examples include Microsoft Agent 365.|
|Coordination Overhead|The additional computational cost, latency, and token consumption incurred by inter-agent communication in multi-agent systems. Research shows overhead scales super-linearly with agent count, creating practical limits on team size. Centralised architectures incur approximately 285% overhead, while hybrid systems can reach 515% relative to single-agent baselines.|
|MAS|Multi-Agent System - an agent architecture comprising multiple LLM-backed agents that communicate through structured message passing, shared memory, or orchestrated protocols. MAS topologies include: Independent (parallel execution with aggregation), Centralised (orchestrator-controlled hierarchy), Decentralised (peer-to-peer debate), and Hybrid (combined hierarchical and lateral communication). Performance is highly task-dependent, with benefits on parallelisable tasks but degradation on sequential reasoning.|
|MCP|The Model Context Protocol (MCP) is an open standard that allows AI models to securely connect with external data sources, tools, and systems through a unified interface, enabling real-time access to information and actions without custom integrations.|
|Orchestration|The coordination and management of multiple AI components, services, or agents to work together in completing complex tasks. Includes workflow management, resource allocation, and inter-component communication.|
|Plan-and-Execute|An agent architecture pattern where planning and execution are separated into distinct phases. The agent first creates a complete plan for solving a task, then executes each step sequentially. Contrasts with ReAct's interleaved approach. Better suited for tasks requiring upfront strategic planning.|
|RAG|Retrieval-Augmented Generation (RAG) is a hybrid approach that combines LLMs with external knowledge retrieval to provide accurate, context-specific answers. The system retrieves relevant documents before generating responses, reducing hallucinations and enabling access to current or proprietary information.|
|ReAct|Reasoning and Acting - an agent framework that interleaves reasoning traces with task-specific actions. The model generates thoughts to plan and track progress, then takes actions to gather information from external sources. Improves interpretability and reduces hallucinations compared to reasoning-only approaches. See: [ReAct Paper](https://arxiv.org/abs/2210.03629)|
|Reasoning Engine|A component that applies logical rules, inference methods, or symbolic reasoning to available knowledge in order to draw conclusions, solve problems, or make decisions. Techniques include rule-based systems, logic programming, and probabilistic reasoning. Used in expert systems, decision support tools, and knowledge graphs.|
|SAS|Single-Agent System - an agent architecture featuring a solitary reasoning locus where all perception, planning, and action occur within a single sequential loop controlled by one LLM instance. SAS has zero communication overhead and can employ tool use, self-reflection, or chain-of-thought reasoning while maintaining computational complexity O(k) where k is the number of reasoning iterations. Research shows SAS often outperforms multi-agent alternatives on tasks requiring sequential constraint satisfaction.|
|Tool Calling|The capability of LLMs to invoke external functions, APIs, or tools during response generation. Enables models to perform actions like web searches, calculations, or database queries. A foundational capability for building AI agents. Also known as "function calling".|
|Workflow Pattern|Reusable architectural patterns for structuring AI agent behaviour. Common patterns include: Prompt Chaining (sequential steps), Parallelisation (concurrent execution), Routing (conditional branching), Orchestrator-Worker (delegated subtasks), and Evaluator-Optimiser (iterative refinement).|

---

## Tooling & Technology

|Term|Definition|
|---|---|
|CLAUDE.md|A configuration file used with Claude Code that contains project-specific instructions, coding standards, and context for the AI assistant. Optimising this file through prompt learning techniques can significantly improve agent performance on coding tasks.|
|Fine-tuning|The process of further training a pre-trained model on a specific dataset or task to improve its performance in a particular domain. Adjusts model weights to specialise capabilities while retaining general knowledge. Requires less data and compute than training from scratch.|
|Prompt Engineering|The practice of designing and refining input prompts to guide a model's responses toward desired outcomes. Goals include improving accuracy, relevance, and creativity of outputs through crafting clear instructions, context, or examples. Structure: CONTEXT (Target, Tone, Limitations) + INSTRUCTIONS (Doing - Concise & Explicit).|
|Prompt Learning|An optimisation approach inspired by reinforcement learning that improves AI agent performance through systematic refinement of system prompts rather than model weight updates. Uses meta-prompting and LLM-based feedback to iteratively improve instructions.|
|SWE-Bench|Software Engineering Benchmark - a standardised evaluation dataset for testing AI coding agents on real-world GitHub issues from popular Python repositories. SWE-Bench Lite contains 300 issues and is commonly used to measure and compare agent performance on software engineering tasks.|
|System Prompt|The initial instructions provided to an LLM that define its behaviour, persona, capabilities, and constraints for a conversation or application. Typically hidden from end users and set by developers to establish the AI's operating parameters.|
|T-C-R|Task + Context + Role - a prompt framework structure for designing effective prompts. T = Task (clearly state what you want), C = Context (provide background or details that shape the response), R = Role (specify the perspective or voice you want the AI to adopt). This structure reduces ambiguity and improves output quality.|

---

## Technical Fundamentals

|Term|Definition|
|---|---|
|Alignment|The challenge of ensuring AI systems behave in accordance with human values, intentions, and goals. Encompasses training techniques, safety measures, and governance approaches to prevent AI from acting in harmful or unintended ways. A central concern in AI safety research.|
|Attention|A mechanism that assigns weights to different input tokens, allowing the model to prioritise important information when processing sequences. Attention enables models to focus on relevant parts of the input regardless of position, forming the core of transformer architectures.|
|Benchmark|A standardised test or dataset used to evaluate and compare AI model performance on specific tasks. Examples include ImageNet (image classification), SWE-Bench (coding), and MMLU (general knowledge). Enables objective measurement of progress and model capabilities.|
|BERT|Bidirectional Encoder Representations from Transformers - a pre-trained language model that reads text in both directions simultaneously, enabling deeper contextual understanding. BERT revolutionised NLP by enabling transfer learning for downstream tasks. See: [BERT Paper](https://arxiv.org/abs/1810.04805)|
|Chain of Thought|A prompting technique that improves LLM reasoning by encouraging step-by-step problem decomposition. Models generate intermediate reasoning traces before arriving at final answers, improving accuracy on complex tasks. Variants include zero-shot CoT ("Let's think step by step") and few-shot CoT with demonstrations. See: [IBM CoT Explainer](https://www.ibm.com/think/topics/chain-of-thoughts)|
|Classification|A supervised learning task where a model learns to assign inputs into predefined categories or classes (e.g., spam vs. not spam, positive vs. negative sentiment). Output is a discrete class label rather than a continuous value.|
|Clustering|An unsupervised learning technique that groups data points into clusters based on similarity, where items in the same cluster are more alike to each other than to those in other clusters. Used for customer segmentation, document grouping, and anomaly detection.|
|Deep Learning|A branch of machine learning using multi-layered neural networks to automatically learn complex patterns and representations from large amounts of data. "Deep" refers to the many hidden layers that progressively extract higher-level features. Powers applications like speech recognition, image classification, and NLP.|
|Embeddings|Dense vector representations of text designed to capture semantic meaning of words, phrases, or documents. By translating textual data into numerical form, embeddings allow models to process and understand relationships between different pieces of text, enabling tasks such as classification, clustering, and retrieval.|
|Few-shot Learning|A machine learning approach where models learn from a small number of examples (typically 2-10) provided in the prompt. Enables task adaptation without fine-tuning by demonstrating the desired behaviour through examples. Contrasts with zero-shot (no examples) and many-shot approaches.|
|GANs|Generative Adversarial Networks - deep learning models comprising two neural networks: a generator that creates synthetic data and a discriminator that evaluates authenticity. Trained together in competition until the generator produces highly realistic outputs. Applications include image synthesis, video generation, and data augmentation.|
|Generative AI|AI models that can create new content such as text, images, audio, or code by learning patterns from existing data and producing outputs that resemble human-generated work. Techniques include transformers, GANs, and diffusion models. Applications span chatbots, image generation, music composition, and creative design.|
|GPT|Generative Pre-trained Transformer - a breakthrough AI architecture with three key components: Generative (can generate text, not just analyse), Pre-trained (trained on massive text data before fine-tuning), and Transformer (neural network architecture excelling at handling sequences and understanding context).|
|Greedy Decoding|A text generation strategy where the model always selects the highest-probability token at each step. Produces deterministic, consistent outputs but can result in repetitive or less creative text. Often used when reproducibility is important. Also known as "temperature 0" sampling.|
|Hallucination|When an AI model generates plausible-sounding but factually incorrect, nonsensical, or fabricated information. This occurs because models predict statistically likely outputs rather than retrieving verified facts. Mitigation strategies include RAG, grounding, and human oversight.|
|Hyperparameter|A configuration value set before training that controls the learning process (e.g., learning rate, batch size, number of layers). Unlike model parameters (weights), hyperparameters are not learned from data. Tuning hyperparameters significantly impacts model performance.|
|In-context Learning|The ability of LLMs to adapt their behaviour based on examples or instructions provided in the prompt, without any weight updates. Enables rapid task switching and customisation through prompt engineering alone. A key emergent capability of large language models.|
|Inference|The process of using a trained model to generate predictions or outputs from new inputs. Contrasts with training (learning from data). Inference speed, cost, and accuracy are key considerations for deploying AI applications in production.|
|JEPA|Joint-Embedding Predictive Architecture - a self-supervised learning framework that learns representations by predicting the embedding of one view of data from another. Developed by Yann LeCun and others as an alternative to contrastive learning methods. See: [LeJEPA Paper](https://arxiv.org/abs/2511.08544)|
|Language Model|A specialised type of machine learning model used for natural language processing (NLP) tasks including sentiment analysis, summarisation, comparative analysis, and generating natural language text.|
|LLM|Large Language Model - a neural network trained on vast amounts of text data, typically containing billions of parameters. LLMs can understand and generate human-like text, perform reasoning tasks, and adapt to various applications through prompting or fine-tuning. Examples include GPT-4, Claude, and Llama.|
|Min-P Sampling|A text generation method that dynamically sets a minimum probability threshold based on the top token's probability. Effectively eliminates low-probability "noise" tokens while preserving high-quality alternatives. Research shows it outperforms Top-P on benchmarks and works well even at high temperatures.|
|ML|Machine Learning - a subset of AI where systems learn patterns from data to make predictions or decisions without being explicitly programmed for each task. Ideal for handling vast amounts of data to make predictions.|
|Monitorability|The property of an AI system that allows observers to reliably predict its behaviour from observable outputs (like Chain of Thought reasoning traces). A more achievable goal than complete "faithfulness" since we can verify behaviour even if we cannot fully understand internal reasoning.|
|Multimodal|AI systems capable of processing and generating multiple types of data (modalities) such as text, images, audio, and video within a single model. Examples include GPT-4V and Gemini. Enables richer interactions and more comprehensive understanding.|
|NLG|Natural Language Generation - the process of automatically producing human-like text or speech from structured data or machine representations. Transforms information into coherent, contextually appropriate language for applications like chatbots, automated reporting, and content creation.|
|NLP|Natural Language Processing - the field that enables computers to understand, interpret, and generate human language in meaningful ways. Core tasks include translation, sentiment analysis, speech recognition, text summarisation, and question answering.|
|Output Drift|Variation in LLM outputs across different runs, providers, or model versions given identical inputs. Creates challenges for auditability and compliance in regulated industries. Research shows smaller models (7-8B parameters) achieve better consistency than larger models (120B+).|
|Parameter|A learned weight or bias in a neural network that shapes how the model transforms inputs into outputs. Weights control the strength of connections between neurones; biases adjust outputs for flexibility. Modern language models can have millions or billions of parameters defining their capabilities.|
|Pre-training|The initial phase of training where a model learns general patterns from large amounts of unlabelled data. Creates foundational capabilities that can be refined through fine-tuning for specific tasks. The "P" in GPT stands for "Pre-trained".|
|Regression|A supervised learning technique used to predict a continuous numerical value based on input data. Models the relationship between variables (features to target value) for applications like predicting house prices, temperature, or stock values.|
|Reinforcement Learning|A training method where an agent learns to make decisions by interacting with an environment, receiving rewards or penalties for actions, and optimising behaviour to maximise cumulative reward. Components include agent, environment, reward signal, and policy (strategy).|
|Self-supervised Learning|A training paradigm where models learn from unlabelled data by creating supervisory signals from the data itself (e.g., predicting masked words, next tokens, or image patches). Enables learning from vast amounts of unlabelled data. Foundation of most modern LLMs.|
|SLM|Small Language Model - a natural language AI model with relatively fewer parameters (millions to a few billion), designed to process and generate text efficiently using less memory and computational power than large language models. Suitable for edge deployment and resource-constrained environments.|
|Softmax|A mathematical function that converts a vector of numbers into a probability distribution, where all values are between 0 and 1 and sum to 1. Used in neural networks to produce output probabilities, particularly for selecting the next token in language models.|
|Supervised Learning|A training approach where a model learns from labelled data - each input is paired with the correct output so it can predict outcomes for new inputs. Applications include spam detection, medical diagnosis, speech recognition, and credit scoring.|
|Temperature|A parameter that controls randomness in LLM text generation by adjusting the probability distribution over possible tokens. Low values (e.g., 0.2) produce more deterministic, focused outputs; high values (e.g., 1.5) increase creativity and diversity. Temperature of 0 equals greedy decoding.|
|Token|A chunk of text (word, subword, or character) that an AI model uses as input for processing language. The smallest unit of text a model processes. For example, "Hello world" might be split into tokens `Hello` and `world`, or smaller subword tokens. Tokens are transformed into numerical representations for computation.|
|Top-K Sampling|A text generation method that restricts token selection to the K highest-probability tokens at each step. Simple but inflexible - a fixed K may be too restrictive in some contexts and too permissive in others. Often combined with temperature for better control.|
|Top-P Sampling|Also called Nucleus Sampling - a text generation method that dynamically selects the smallest set of tokens whose cumulative probability exceeds a threshold P. More adaptive than Top-K as the number of considered tokens varies based on the model's confidence. Common values range from 0.9 to 0.95.|
|Transformer|A neural network architecture that processes sequential data using self-attention mechanisms, enabling parallel processing and capturing long-range dependencies. Forms the foundation of modern language models. See: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)|
|Unsupervised Learning|A training approach where a model learns patterns and structures from unlabelled data without predefined outputs, by grouping, clustering, or reducing dimensions to reveal hidden relationships. Applications include customer segmentation, anomaly detection, and data compression.|
|Vector Database|A specialised database designed to store, index, and query high-dimensional vector embeddings efficiently. Enables similarity search operations to find semantically related content, forming a critical component of RAG systems and semantic search applications.|
|ViT|Vision Transformer - an architecture that applies the transformer model (originally designed for text) to image classification tasks. Divides images into patches, treats them as token sequences, and processes them with self-attention. Achieves state-of-the-art results on many computer vision benchmarks.|
|Zero-shot Learning|A machine learning approach where models perform tasks without any task-specific examples in the prompt. Relies entirely on the model's pre-trained knowledge and instruction-following capabilities. Contrasts with few-shot learning where examples are provided.|

---

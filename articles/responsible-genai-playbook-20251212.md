# Responsible Use of Generative AI: A Playbook for Product Managers & Business Leaders
**Authors:** Genevieve Smith, Natalia Luka, Jessica Newman, Merrick Osborne, Brandie Nonnecke (UC Berkeley); Brian Lattimore (Stanford University); Brent Mittelstadt (University of Oxford)

**Category:** Strategy & Governance

**Published:** 2025

---

## Key Points
- The playbook provides 10 practical "plays" split between organisational leaders (5 plays) and product managers (5 plays) for implementing responsible GenAI practices across the enterprise.
- Five core responsibility risks must be addressed: data privacy, transparency, inaccuracy/hallucinations, bias, and safety/security vulnerabilities including prompt injection attacks.
- GenAI high performers achieve success by proactively addressing risks through clear governance frameworks, leadership commitment, and comprehensive training programmes.
- Organisations face significant challenges including lack of policies, misaligned incentives, insufficient education, and general industry immaturity around responsibility practices.
- The business case for responsible AI includes building trust and brand reputation (57% of consumers uncomfortable with data use), maintaining regulatory compliance (EU AI Act fines up to 7% of turnover), and mitigating risks for sustainable growth.

---

## Summary
This comprehensive playbook from UC Berkeley's BAIR Responsible AI Initiative provides practical guidance for product managers and business leaders navigating the responsible use of generative AI. Developed through academic research involving 25 interviews and 300 survey respondents, the playbook addresses the rapid adoption of GenAI across industries—where 65% of organisations now regularly use these technologies, nearly double from ten months prior. The framework is designed to help organisations unlock GenAI's potential whilst embedding trust and fostering accountability through proactive risk management.

The playbook identifies five critical responsibility risks: data privacy concerns (including unintended data exposure and copyright infringement), transparency challenges from "black box" model architectures, hallucinations and inaccuracy (with rates ranging from 3-27% depending on models), bias issues that both disadvantage certain populations and reinforce harmful stereotypes, and safety/security vulnerabilities like prompt injection attacks. Beyond these technical risks, the document examines broader implications including environmental impacts, future of work concerns, and copyright infringement. Organisational challenges include the lack of clear policies (many organisations still grappling with appropriate use), misaligned incentives prioritising speed over responsibility, insufficient individual education, and general industry immaturity around responsibility practices.

The solution framework comprises 10 sequential "plays": five for organisational leaders (ensuring leadership commitment and developing responsible AI principles, implementing policies and standards, building comprehensive governance frameworks, updating incentives to align with responsibility, and implementing tailored training) and five for product managers (conducting "gut checks" for risk evaluation, choosing appropriate models with transparency documentation, conducting risk assessments and audits, implementing red-teaming and adversarial testing, and tracking responsibility "micro-moments" in performance reviews). The playbook emphasises that organisations excelling with GenAI are those paying attention to risks—GenAI high performers use the technology in multiple business functions and follow risk-related best practices, whilst organisations lagging in addressing risks are inhibited from capitalising on benefits.

---
**Captured:** 12-Dec-2025

**Link:** https://re-ai.berkeley.edu/sites/default/files/responsible_use_of_generative_ai_uc_berkeley_2025.pdf

**Key Words:** Responsible AI; Governance; Product Management; Risk Management; Business Strategy
